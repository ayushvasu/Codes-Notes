s-c2r {

  # Spark config
  # ~~~~~~~~~~~~
  spark {
          "spark.app.name" = "s-c2r"
          "spark.cassandra.connection.host"=""
//          "spark.cassandra.connection.keep_alive_ms" = "20000"
         }
  # app parameters
  dataframe-merge-batch-size = 200
  max-etl-time = 24 hours
  start-rx-date = "2017-05-30T00:00Z"
  end-rx-date = "2017-05-30T00:00Z"
  manual-run = true
  update-loaded = true
  
  # Data Sources 
  # ~~~~~~~~~~~~
  data-sources {
    
    src {
      format = "org.apache.spark.sql.cassandra"
    }
    
    dst {
      format = "com.vertica.spark.datasource.DefaultSource"
      mode = "append"      
      options {
          db = "database"
          strlen = "4096"
          user = "gbd"
          password = ""
          tmpdir = "/ebs/spark/tmp"
          hdfs_url = "hdfs://:8020/user/gbd/S2V"
          host = ""
       }
    }
    
  }
    
  cassandra {
    seeds = [""]
    driver-protocol-version = 4

    # query options    
    read-consistency-level = "QUORUM"
    fetch-size = 1000

    # socket options    
    driver-read-timeout = 50 seconds
    driver-connect-timeout = 30 seconds
    driver-tcp-no-delay = true

    # connection pooling options    
    core-connections-per-host-local = 1
    max-connections-per-host-local = 1
    core-connections-per-host-remote = 1
    max-connections-per-host-remote = 1
    max-requests-per-connection-local= 10240
    max-requests-per-connection-remote= 10240
    connection-max-idle-time-seconds = 120 
    connection-max-queue-size = 256
    connection-pool-timeout-millis = 5000
    connection-heartbeat-interval-seconds = 30

    # reconnect options    
    driver-reconnect-base-delay = 1 seconds
    driver-reconnect-max-delay = 60 seconds


    read-retries = 3
    read-delay-between-retries = 20 seconds
    read-retries-within-time = 1 minute
  }
}

    
